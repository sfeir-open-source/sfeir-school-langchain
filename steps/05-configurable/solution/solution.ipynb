{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../../img/installation-ico.png) Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain langchain-community langchain-google-vertexai google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../../img/package-ico.png) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.auth\n",
    "\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.base import BaseCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../../img/parametrage-ico.png) Paramétrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion credentials VertexAI\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../../settings/credentials.json\"\n",
    "google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Any, Dict\n",
    "models = [ \"gemini-1.5-pro\", \"text-bison\", \"text-unicorn\" ]\n",
    "\n",
    "class MyHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts, metadata, **kwargs\n",
    "    ):\n",
    "        print(f\"on_llm_start {metadata['name']}\")\n",
    "        \n",
    "    def on_llm_end(\n",
    "        self, response, **kwargs\n",
    "    ):\n",
    "        print(f\"on_llm_end\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "        print(\"Got error while running :\", error)\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], metadata, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {metadata['name']}\")\n",
    "        \n",
    "    def on_chain_end(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], metadata, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_end {metadata['name']}\")\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "llms = [ VertexAI(model_name=m, callbacks=[MyHandler()], metadata={\"name\": m}) for m in models]\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {input}.\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chains = [prompt | llm | output_parser for llm in llms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../../img/jouer-ico.png) Exécution(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"elephant\"\n",
    "for c in chains:\n",
    "    for s in c.invoke({\"input\": input}):\n",
    "        print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurable alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI, VertexAI\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    temperature=0\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"latest\",\n",
    "    # Ajout des autres options associées à d'autre clé\n",
    "    gemini_old=ChatVertexAI(model_name=\"gemini-1.0-pro\"),\n",
    "    legacy=VertexAI(model_name=\"text-bison\")\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"En tant qu'assistant IA, avant toute réponse indique absolument qui tu es et le nom exact de ton modèle.\n",
    "                  Uniquement et seulement après, tu peux répondre à la demande de l'utilisateur.\n",
    "                  Ta réponse sera sous la forme suivante :\n",
    "                  Modèle : 'Nom du modèle utilisé'\n",
    "                  Réponse : 'Réponse à la demande'\n",
    "                  \"\"\"),\n",
    "    HumanMessage(content=\"Parle moi de LangChain.\")\n",
    "    ])\n",
    "chain = prompt | llm\n",
    "\n",
    "configs = [{\"llm\": model_key} for model_key in [\"latest\", \"gemini_old\", \"legacy\"]]\n",
    "for c in configs:\n",
    "    print(\"Calling with \", c)\n",
    "    print(chain.with_config(configurable=c).invoke({}))\n",
    "    print(\"------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvSchool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
