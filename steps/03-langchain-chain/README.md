# Langchain - Chain

## Objectif

Pour ce lab, l'id√©e est de construire et de se familiriaser avec le principe de LCEL et de construire sa premi√®re cha√Æne.

```mermaid
flowchart LR
    A[Prompt] --> B[LLM]
    B --> C[Parser de String]
```

## Etapes

Sur la base du pr√©c√©dent lab, d√©finir un llm reli√© √† un mod√®le Ollama, par exemple "mistral".

Pour la cha√Æne on va rajouter un interpr√©teur et une nouvelle sortie de r√©ponse pour embellir celle-ci.

<!-- ![schema](./assets/schema.png) -->

On va avoir besoin d'un template pour structurer le prompt que l'on va transmettre, **PromptTemplate**, du package `langchain_core.prompts`, va permettre de d√©finir un template de notre input de messages. Il poss√®de une m√©thode *`from_template`* qui va permetre via les attributs entre '{}' d'√™tre inject√© via notre cha√Æne d'interpr√©tation.
Et pour la sortie afin d'avoir une r√©ponse plus embellie, nous allons utiliser le parser *`StrOutputParser`*, du package `langchain_core.output_parsers`, sans options particuli√®res.

Le cha√Ænage LCEL se fait via l'operateur **'|'** entre les objets, que l'on affecte √† une variable. Et ainsi appeler la m√©thode invoke avec les param√®tres souhait√©s, correspondant en particulier au param√®tres attendus d√©finit dans notre template de prompt d√©finit.

Vous devez arriv√© √† une √©criture proche de celle-ci :

```python
chain = prompt | llm | output_parser
chain.invoke({"input":input})
```

## Aller plus loin

### Streaming

On peut stream la r√©ponse, au lieu d'appeler la m√©thode *`invoke`*, utilisez la m√©thode *`stream()`*. 
Attention, cette m√©thode vous retourne un it√©rateur sur les morceaux de r√©ponses quand ils sont disponibles, donc il va falloir boucler dessus et utiliser la m√©thode *`print()`* avec les options suivantes ```end="", flush=True```.

### ChatPrompt

```mermaid
flowchart TD
    SM1[Message syst√®me - contexte]
    SM2[Message syst√®me - instructions JSON]
    HM[Human Message]
    J(Structure JSON)
    P[Parser JSON]
    L[LLM]

    J -.-> SM2
    J -.-> P

    subgraph prompt
        SM1 --> SM2 --> HM
    end

    prompt --> L --> P
```

On peut trouver des prompts plus avanc√©s tout comme les parsers. Essayez avec pour prompt : **ChatPromptTemplate** et pour parser : **JsonOutputParser**. Pour le second s'ajoute une complexit√©, o√π il va falloir donner dans le prompt, l'information que l'on souhaite la r√©ponse dans un certain format, pour permettre l'interpr√©tation et la structuration de la r√©ponse correctement.

* D√©j√† structurez la r√©ponse, en se basant sur la lib [**pydantic**](https://docs.pydantic.dev/1.10/)

```python
from langchain_core.pydantic_v1 import BaseModel, Field

class Response(BaseModel):
    answer: str = Field(description="answer to the user's question")
    source: str = Field(description="source used to answer the user's question, should be a website")
```

D√©finissons notre parser de sortie JSON en prenant en compte la structure attendue, en utilisant **JsonOutputParser** avec le param√®tre *`pydantic_object`*.

Ensuite, on peut d√©finir notre prompt en tant que **ChatPromptTemplate**. Au sein du prompt ne pas oublier d'ajouter un message de type **"system"**, permettant d'injecter les instructions du format de r√©ponse. Ajouter en dernier message l'input de message qui va nous permettre de prendre en compte l'entr√©e au moment du lancement de la cha√Æne.

> **Astuce üí°:** **JsonOutputParser** a une m√©thode *`get_format_instructions()`* permettant de r√©cup√©rer le format de l'objet sous forme d'instruction.

Cha√Æner les √©l√©ments et ex√©cuter :

* le prompt de chat
* le llm d√©signant le model
* le parser de sortie

```python
chain = prompt | llm | parser
chain.invoke({"question": "What is the capital of Norway?"})

# {'answer': 'Oslo', 'source': 'https://en.wikipedia.org/wiki/Capital_city'}
```
