{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain / LLMS - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../img/installation-ico.png) Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../img/package-ico.png) Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.base import BaseCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../img/parametrage-ico.png) Paramétrages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Any, Dict\n",
    "models = [ \"llama2\", \"mistral\", \"codellama\" ]\n",
    "\n",
    "class MyHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts, metadata, **kwargs\n",
    "    ):\n",
    "        print(f\"\\non_llm_start {metadata['name']}\")\n",
    "        \n",
    "    def on_llm_end(\n",
    "        self, response, **kwargs\n",
    "    ):\n",
    "        print(f\"on_llm_end\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], metadata, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {metadata['name']}\")\n",
    "        \n",
    "    def on_chain_end(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], metadata, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_end {metadata['name']}\")\n",
    "\n",
    "llms = [ Ollama(model=m, callbacks=[MyHandler()], metadata={\"name\": m}) for m in models]\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a joke about {input}.\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chains = [prompt | llm | output_parser for llm in llms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](../img/jouer-ico.png) Exécution(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"elephant\"\n",
    "for c in chains:\n",
    "    for s in c.invoke({\"input\": input}):\n",
    "        print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama().configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"llama2\",\n",
    "    # Ajout des autres options associées à d'autre clé\n",
    "    mistral=ChatOllama(model=\"mistral\"),\n",
    "    codellama=ChatOllama(model=\"codellama\"),\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"En tant qu'assistant IA, avant toute réponse indique absolument qui tu es et le nom de ton modèle.\n",
    "                  Uniquement et seulement après, tu peux répondre à la demande de l'utilisateur\n",
    "                  \"\"\"),\n",
    "    HumanMessage(content=\"Parle moi de LangChain.\")\n",
    "    ])\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.invoke({}), end=\"\\n\")\n",
    "\n",
    "configs = [{\"llm\": m} for m in [\"mistral\", \"llama2\", \"codellama\"]]\n",
    "for c in configs:\n",
    "    print(chain.with_config(configurable=c).invoke({}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
