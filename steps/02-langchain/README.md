# Langchain

## Objectifs

Les objectifs de ce lab sont de r√©aliser les m√™mes appels au llm **llama2**, via langchain

* Installer les librairies python langchain
* installer les librairies compl√©mentaires au client du LLM
* renseigner son premier prompt
* d√©finir un premier parser de sortie

![INFO](../img/info.png) Pour l'ensemble des lab, il est recommand√© de travailler sur un environnement virtuel python. Il est m√™me possible de travailler avec Jupiter directement int√©gr√© dans votre IDE pour une pleine pratique des labs.

## Recommendation

Pour l'ensemble des labs qui vont suivre, **il est recommand√© de d√©finir un environnement virtuel** que vous allez utiliser pour ex√©cuter tous les labs qui vont suivre.
Pour ce faire dans votre espace de travail ex√©cuter la commande suivante :

```sh
python3 -m venv .school.venv
```

Par la suite si vous revenez dans cet espace, n'oubliez pas de r√©activer l'environnement virtuel via la commande suivante :

```sh
source .school.venv/bin/activate
```
> **Astuce üí°**: Lorsque l'environement virtuel est activ√© dans la session courante de votre terminal, son **nom devrait apparaitre au d√©but de chaque ligne**, comme suit (ceci est exemple):
> 
> **(.school.venv)** node ‚ûú /workspaces/sfeir-school-langchain (main) $ 

## Installation

Via **pip**, installer les librairies `langchain` et `langchain-community`. Celles-ci comprennent respectivement les modules/outils core pour cr√©√©r des applications langchain, ainsi que les outils et extensions communautaires, comme les int√©grations avec les LLMs `Ollama`, `OpenAI`, etc...

```sh
pip install langchain langchain-community
```

## Mon Premier script LangChain

Comme premier LLM, nous allons utiliser le serveur ollama pr√©c√©demment install√©/utilis√©.
Pour notre premier script python, nous allons avoir besoin de la librairie _Ollama_ pour se connecter au LLM local.
Cette librairie est incluse dans le module `langchain-community`.

```python
from langchain_community.llms.ollama import Ollama
```

Ensuite, d√©finissez une variable de d√©finition de notre model **llama** (ou "mistral", un model pr√©c√©demment pull dans ollama), en utilisant le constructeur _Ollama()_. Cet objet va se charger d'int√©rargir au travers du serveur Ollama avec un LLM, en transmettant le prompt souhait√©.

Pour se faire, l'objet d√©finit par langchain fournit une m√©thode _invoke_ pour d√©clencher une s√©quence.

**Remarque :** S'assurer que le serveur _Ollama_ est toujours actif. Le temps de r√©ponse d√©pendra fortement de votre carte graphique et/ou si le serveur _Ollama_ tourne directement sur votre machine ou dans une image Docker (tr√®s lent).
